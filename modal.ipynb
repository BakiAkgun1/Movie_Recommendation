{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering # type: ignore\n",
    "from sklearn.mixture import GaussianMixture # type: ignore\n",
    "from sklearn.cluster import DBSCAN # type: ignore\n",
    "from sklearn.metrics import silhouette_score # type: ignore\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # type: ignore\n",
    "import numpy as np # type: ignore\n",
    "import pandas as pd # type: ignore\n",
    "import matplotlib.pyplot as plt # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       culture clash future space war space colony so...\n",
       "1       ocean drug abuse exotic island east india trad...\n",
       "2       spy based novel secret agent sequel mi action ...\n",
       "3       dc comics crime fighter terrorist secret ident...\n",
       "4       based novel mars medallion space travel prince...\n",
       "                              ...                        \n",
       "4798    united states umexico barrier legs arms paper ...\n",
       "4799    unknown comedy romance newlywed couple honeymo...\n",
       "4800    date love first sight narration investigation ...\n",
       "4801    unknown drama ambitious new york attorney sam ...\n",
       "4802    obsession camcorder crush dream girl documenta...\n",
       "Name: combined_features, Length: 4801, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv(\"clean_data.csv\")\n",
    "data =  data.dropna(subset=['keywords', 'genres','overview'])\n",
    "\n",
    "data['combined_features'] = data['keywords'] + ' ' + data['genres'] + ' ' + data['overview']\n",
    "data['combined_features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidfVectorizer ile metni sayısal veriye dönüştürme\n",
    "tfidf = TfidfVectorizer(stop_words ='english')\n",
    "tfidf_matrix = tfidf.fit_transform(data['combined_features'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hieararchical Clustering \n",
    "hier_clustering = AgglomerativeClustering(n_clusters= 10)\n",
    "hier_labels = hier_clustering.fit_predict(tfidf_matrix.toarray())\n",
    "silhouette_hier= silhouette_score(tfidf_matrix, hier_labels)\n",
    "print(f\"Hierarchical Clustering Silhoutte Score: {silhouette_hier}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBSCAM Silhoutte Score: -1\n"
     ]
    }
   ],
   "source": [
    "#DBSCAN\n",
    "dbscan = DBSCAN(eps = 0.5, min_samples=5)\n",
    "dbscan_labels = dbscan.fit_predict(tfidf_matrix.toarray())\n",
    "\n",
    "if len(set(dbscan_labels)) > 1 and -1 not in dbscan_labels:\n",
    "    silhoutte_dbscan = silhouette_score(tfidf_matrix, dbscan_labels)\n",
    "else:\n",
    "    silhoutte_dbscan = -1 #dbscan başarısızz  ise  -1 yazalım\n",
    "print(f\"DBSCAN Silhoutte Score: {silhoutte_dbscan}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GM Silhoutte Score: -0.010837713742997463\n"
     ]
    }
   ],
   "source": [
    "#Gaussian Mixture Model(GMM\n",
    "\n",
    "from sklearn.decomposition import PCA # type: ignore\n",
    "pca = PCA(n_components=100)\n",
    "tfidf_matrix_reduced = pca.fit_transform(tfidf_matrix.toarray())\n",
    "\n",
    "gmm = GaussianMixture(n_components=10, random_state=42)\n",
    "gmm_labels= gmm.fit_predict(tfidf_matrix_reduced)\n",
    "silhouette_gmm = silhouette_score(tfidf_matrix_reduced, gmm_labels)\n",
    "print(f\"GM Silhoutte Score: {silhouette_gmm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results\n",
    "clustering_methods = ['Hierarchical','DBSCAN','GMM']\n",
    "silhouette_scores = [silhouette_hier, silhoutte_dbscan, silhouette_gmm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_method_index = np.argmax(silhouette_scores)\n",
    "best_method = clustering_methods[best_method_index]\n",
    "best_score = silhouette_scores[best_method_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En iyi kümeleme yöntemi: Hierarchical\n",
      "En yüksek Silhouette Skoru: -0.0030384601312127874\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"En iyi kümeleme yöntemi: {best_method}\")\n",
    "print(f\"En yüksek Silhouette Skoru: {best_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import fpgrowth, association_rules,apriori # type: ignore\n",
    "from sklearn.feature_extraction.text import CountVectorizer # type: ignore\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FPGrowth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Baki Akgun\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(tokenizer = lambda x: x.split(' '), binary=True)\n",
    "X = vectorizer.fit_transform(data['combined_features'])\n",
    "features  = vectorizer.get_feature_names_out()\n",
    "\n",
    "df_features = pd.DataFrame(X.toarray(), columns = features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Baki Akgun\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sık kullanılan Ögeler:       support                     itemsets\n",
      "0    0.249948                     (action)\n",
      "1    0.173089                  (adventure)\n",
      "2    0.113726                    (science)\n",
      "3    0.112477                    (fiction)\n",
      "4    0.090606                    (fantasy)\n",
      "..        ...                          ...\n",
      "535  0.047282          (independent, film)\n",
      "536  0.035618         (independent, drama)\n",
      "537  0.024995        (independent, comedy)\n",
      "538  0.034160   (independent, drama, film)\n",
      "539  0.024162  (independent, comedy, film)\n",
      "\n",
      "[540 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "frequent_itemsets = fpgrowth(df_features, min_support = 0.02, use_colnames = True)\n",
    "rules = association_rules(frequent_itemsets, metric = \"lift\", min_threshold = 1)\n",
    "print(f\"Sık kullanılan Ögeler: {frequent_itemsets}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Birliktelik Kuralları:              antecedents            consequents   support  confidence  \\\n",
      "0             (thriller)               (action)  0.112268    0.424076   \n",
      "1               (action)             (thriller)  0.112268    0.449167   \n",
      "2      (thriller, drama)               (action)  0.034368    0.295699   \n",
      "3        (drama, action)             (thriller)  0.034368    0.460894   \n",
      "4             (thriller)        (drama, action)  0.034368    0.129819   \n",
      "..                   ...                    ...       ...         ...   \n",
      "649  (independent, film)               (comedy)  0.024162    0.511013   \n",
      "650       (comedy, film)          (independent)  0.024162    0.588832   \n",
      "651        (independent)         (comedy, film)  0.024162    0.489451   \n",
      "652             (comedy)    (independent, film)  0.024162    0.067052   \n",
      "653               (film)  (independent, comedy)  0.024162    0.240664   \n",
      "\n",
      "          lift  \n",
      "0     1.696656  \n",
      "1     1.696656  \n",
      "2     1.183042  \n",
      "3     1.740953  \n",
      "4     1.740953  \n",
      "..         ...  \n",
      "649   1.418136  \n",
      "650  11.928206  \n",
      "651  11.928206  \n",
      "652   1.418136  \n",
      "653   9.628562  \n",
      "\n",
      "[654 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n  Birliktelik Kuralları: {rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Baki Akgun\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "data['combined_features'] = data['combined_features'].apply(lambda x: x.split())\n",
    "vectorizer = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False, binary=True)\n",
    "X = vectorizer.fit_transform(data['combined_features'])\n",
    "df_features = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [culture, clash, future, space, war, space, co...\n",
       "1    [ocean, drug, abuse, exotic, island, east, ind...\n",
       "2    [spy, based, novel, secret, agent, sequel, mi,...\n",
       "3    [dc, comics, crime, fighter, terrorist, secret...\n",
       "4    [based, novel, mars, medallion, space, travel,...\n",
       "Name: combined_features, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['combined_features'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Baki Akgun\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sık  ullanılan Ögeler:       support                               itemsets\n",
      "0    0.249948                               (action)\n",
      "1    0.173089                            (adventure)\n",
      "2    0.022704                                  (age)\n",
      "3    0.034368                                (agent)\n",
      "4    0.027286                                (along)\n",
      "..        ...                                    ...\n",
      "535  0.020204                   (true, drama, story)\n",
      "536  0.042908           (science, fiction, thriller)\n",
      "537  0.027494              (science, fiction, world)\n",
      "538  0.030619  (science, fiction, adventure, action)\n",
      "539  0.025828   (science, fiction, action, thriller)\n",
      "\n",
      "[540 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "frequent_itemsets = apriori(df_features, min_support= 0.02, use_colnames= True)\n",
    "\n",
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold= 1)\n",
    "\n",
    "print(f\"Sık  ullanılan Ögeler: {frequent_itemsets}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Association Rules:             antecedents                   consequents   support  confidence  \\\n",
      "0           (adventure)                      (action)  0.099354    0.574007   \n",
      "1              (action)                   (adventure)  0.099354    0.397500   \n",
      "2               (agent)                      (action)  0.022495    0.654545   \n",
      "3              (action)                       (agent)  0.022495    0.090000   \n",
      "4              (action)                       (based)  0.024162    0.096667   \n",
      "..                  ...                           ...       ...         ...   \n",
      "649  (thriller, action)            (science, fiction)  0.025828    0.230056   \n",
      "650           (science)   (thriller, fiction, action)  0.025828    0.227106   \n",
      "651           (fiction)   (science, action, thriller)  0.025828    0.229630   \n",
      "652            (action)  (science, fiction, thriller)  0.025828    0.103333   \n",
      "653          (thriller)    (science, fiction, action)  0.025828    0.097561   \n",
      "\n",
      "         lift  \n",
      "0    2.296507  \n",
      "1    2.296507  \n",
      "2    2.618727  \n",
      "3    2.618727  \n",
      "4    1.097155  \n",
      "..        ...  \n",
      "649  2.076123  \n",
      "650  8.722696  \n",
      "651  8.890741  \n",
      "652  2.408269  \n",
      "653  1.678818  \n",
      "\n",
      "[654 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n Association Rules: {rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
